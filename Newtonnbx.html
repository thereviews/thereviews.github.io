<!doctype html>

<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover" />
  <title>AI Boxing Coach (HTML + JS)</title>
  <style>
    :root{--accent:#1f7aea}
    body{font-family:system-ui,-apple-system,Segoe UI,Roboto,Arial;margin:0;background:#0b0c0f;color:#e6eef8}
    header{padding:12px;background:linear-gradient(90deg,#071033, #0b1226);display:flex;gap:10px;align-items:center}
    header h1{font-size:16px;margin:0}
    #app{display:grid;grid-template-columns:1fr 320px;gap:10px;height:calc(100vh - 56px);padding:10px}
    .stage{position:relative;border-radius:8px;overflow:hidden;background:#000;min-height:360px}
    video{width:100%;height:100%;object-fit:cover;transform:scaleX(-1)}
    canvas.overlay{position:absolute;inset:0;touch-action:none}
    .panel{background:linear-gradient(180deg,#071229,#071629);padding:12px;border-radius:8px;overflow:auto}
    .status{display:flex;gap:8px;flex-wrap:wrap}
    .badge{background:rgba(255,255,255,0.04);padding:6px 8px;border-radius:6px;font-size:13px}
    label{display:block;margin-top:10px;font-size:13px}
    input[type=range]{width:100%}
    button{background:var(--accent);border:0;padding:8px 10px;border-radius:8px;color:white;font-weight:600}
    .small{font-size:13px;color:#b9c7dd}
    .metric{display:flex;justify-content:space-between;padding:6px 4px;border-bottom:1px dashed rgba(255,255,255,0.03)}
    footer{padding:8px 12px;font-size:12px;color:#9fb3d9}
    @media(max-width:900px){#app{grid-template-columns:1fr}}
  </style>
</head>
<body>
  <header>
    <h1>AI Boxing Coach — Front Camera (Web)</h1>
    <div class="small">Run in Chrome on Android for best results</div>
  </header>  <div id="app">
    <div class="stage">
      <video id="video" autoplay playsinline muted></video>
      <canvas id="overlay" class="overlay"></canvas>
    </div><div class="panel">
  <div class="status">
    <div class="badge" id="ready">Model: loading...</div>
    <div class="badge" id="fps">FPS: --</div>
    <div class="badge" id="punchCount">Punches: 0</div>
    <div class="badge" id="lastFeedback">Feedback: —</div>
  </div>

  <label>Sensitivity: <span id="sensVal">0.8</span></label>
  <input id="sens" type="range" min="0.4" max="1.6" step="0.05" value="0.8">

  <label>Power multiplier: <span id="powVal">1.0</span></label>
  <input id="pow" type="range" min="0.4" max="2.5" step="0.05" value="1.0">

  <label>Target (toggle) - aim at the blue circle</label>
  <input id="showTarget" type="checkbox" checked>

  <div style="margin-top:10px;display:flex;gap:8px">
    <button id="startBtn">Start</button>
    <button id="stopBtn" disabled>Stop</button>
  </div>

  <h3 style="margin-top:12px">Realtime metrics</h3>
  <div class="metric"><div>Last punch speed</div><div id="speed">0.00</div></div>
  <div class="metric"><div>Estimated power</div><div id="power">0.00</div></div>
  <div class="metric"><div>Accuracy</div><div id="accuracy">—</div></div>
  <div class="metric"><div>Guard state</div><div id="guardState">—</div></div>
  <div class="metric"><div>Chin exposed</div><div id="chinExposed">—</div></div>

  <h3 style="margin-top:12px">Notes</h3>
  <div class="small">This demo uses a browser pose model to estimate wrists, shoulders and head. It gives heuristic feedback (speed, power, accuracy, guard, chin). Not perfect — use as training aid only.</div>

</div>

  </div>  <footer>Allow camera → put phone on a stand → face the camera. Keep hands visible.</footer>  <script>
  // ---------- SETTINGS ----------
  const CONFIG = {
    video: {width: 720, height: 1280, facingMode: 'user'},
    smoothing: 0.7,            // lowpass smoothing for positions
    speedWindowMs: 160,       // time window to estimate speed
    punchThreshold: 0.55,     // normalized forward-extension threshold
    guardDistThreshold: 0.18, // fraction of screen diag
    chinAngleThreshold: 0.18, // threshold for chin-exposed
    fpsSmoothing: 0.9
  };
<script>
  // ---------- UI ----------
  const video = document.getElementById('video');
  const canvas = document.getElementById('overlay');
  const ctx = canvas.getContext('2d');
  const startBtn = document.getElementById('startBtn');
  const stopBtn = document.getElementById('stopBtn');
  const readyBadge = document.getElementById('ready');
  const fpsBadge = document.getElementById('fps');
  const punchCountBadge = document.getElementById('punchCount');
  const lastFeedback = document.getElementById('lastFeedback');

  const speedElem = document.getElementById('speed');
  const powerElem = document.getElementById('power');
  const accuracyElem = document.getElementById('accuracy');
  const guardElem = document.getElementById('guardState');
  const chinElem = document.getElementById('chinExposed');

  const sens = document.getElementById('sens');
  const pow = document.getElementById('pow');
  const sensVal = document.getElementById('sensVal');
  const powVal = document.getElementById('powVal');
  const showTarget = document.getElementById('showTarget');

  sens.addEventListener('input', ()=> sensVal.textContent = sens.value);
  pow.addEventListener('input', ()=> powVal.textContent = pow.value);

  let detector = null;
  let rafId = null;
  let running = false;

  // simple audio feedback with WebAudio
  const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
  function beep(freq=440,dur=0.12, type='sine'){
    try{
      const o = audioCtx.createOscillator();
      const g = audioCtx.createGain();
      o.type = type; o.frequency.value = freq; o.connect(g); g.connect(audioCtx.destination);
      g.gain.setValueAtTime(0.0001, audioCtx.currentTime);
      g.gain.exponentialRampToValueAtTime(0.2, audioCtx.currentTime + 0.01);
      o.start();
      g.gain.exponentialRampToValueAtTime(0.0001, audioCtx.currentTime + dur);
      o.stop(audioCtx.currentTime + dur + 0.02);
    }catch(e){console.warn('Audio err',e)}
  }

  // ---------- Load TensorFlow pose model (MoveNet) ----------
  // Using lightweight MoveNet via @tensorflow-models/pose-detection (CDN loaded below)
  // The page will download model files at runtime.

  // Create a tiny state to track positions history
  const state = {
    lastTs: performance.now(),
    history: [], // {ts, leftWrist:{x,y}, rightWrist...}
    punchCount: 0,
    lastPunchTs: 0,
    lastFPS: 0,
    fps: 0
  };

  function sizeCanvas(){
    const rect = video.getBoundingClientRect();
    canvas.width = rect.width;
    canvas.height = rect.height;
  }

  async function startCamera(){
    const stream = await navigator.mediaDevices.getUserMedia({video:{facingMode:CONFIG.video.facingMode}});
    video.srcObject = stream;
    await video.play();
    sizeCanvas();
  }

  // Utility: linear interpolation for smoothing
  function lerp(a,b,t){return a + (b-a)*t}

  // Map pose keypoints to easy-access object with normalized coords (0..1)
  function normKeypoint(k){
    if(!k) return null;
    return {x: k.x / video.videoWidth, y: k.y / video.videoHeight, score: k.score ?? k.score ?? 1};
  }

  // Compute euclidean distance in normalized coords
  function ndist(a,b){ if(!a||!b) return 999; const dx=a.x-b.x, dy=a.y-b.y; return Math.sqrt(dx*dx+dy*dy); }

  // compute velocity between two points over dt seconds
  function velocity(p1,p2,dt){ if(!p1||!p2||dt<=0) return 0; const dx = (p2.x-p1.x), dy = (p2.y-p1.y); const dist = Math.sqrt(dx*dx+dy*dy); return dist/dt; }

  // main analysis heuristics
  function analyze(pose,now){
    const kp = {};
    for(const k of pose.keypoints){ kp[k.name] = {x:k.x, y:k.y, score:k.score ?? 1}; }
    // normalize to video dims
    const toNorm = (p)=>p?{x:p.x/video.videoWidth, y:p.y/video.videoHeight, score:p.score}:null;
    const leftW = toNorm(kp['left_wrist']);
    const rightW = toNorm(kp['right_wrist']);
    const leftS = toNorm(kp['left_shoulder']);
    const rightS = toNorm(kp['right_shoulder']);
    const nose = toNorm(kp['nose']);
    const leftEye = toNorm(kp['left_eye']);
    const rightEye = toNorm(kp['right_eye']);

    const midShoulder = {x:(leftS?.x ?? rightS?.x ?? 0.5)/1, y:(leftS?.y ?? rightS?.y ?? 0.4)/1};

    // push history
    state.history.push({ts: now, leftW, rightW, leftS, rightS, nose, midShoulder});
    // keep only recent
    const cutoff = now - CONFIG.speedWindowMs;
    while(state.history.length && state.history[0].ts < cutoff) state.history.shift();

    // Estimate hand speeds (norm units per sec)
    const first = state.history[0];
    const last = state.history[state.history.length-1];
    const dt = Math.max(0.001, (last.ts - first.ts)/1000);
    const leftSpeed = velocity(first.leftW, last.leftW, dt);
    const rightSpeed = velocity(first.rightW, last.rightW, dt);
    const handSpeed = Math.max(leftSpeed, rightSpeed);

    // Estimate forward extension: compare wrist x distance from shoulder center; because we mirror video, forward direction roughly corresponds to decreasing x from shoulder? We'll use projection on vector from shoulder to wrist.
    // Simpler: compute extension = distance between wrist and shoulder normalized by shoulder width
    const shoulderWidth = ndist(leftS, rightS) || 0.2;
    const leftExt = leftS && leftW ? ndist(leftS,leftW)/shoulderWidth : 0;
    const rightExt = rightS && rightW ? ndist(rightS,rightW)/shoulderWidth : 0;
    const ext = Math.max(leftExt,rightExt);

    // Punch detection: significant increase in speed + extension beyond threshold
    const speedThreshold = 0.6 * parseFloat(sens.value);
    const extThreshold = CONFIG.punchThreshold * (1/parseFloat(sens.value));
    let punchDetected = false;
    if(handSpeed > speedThreshold && ext > extThreshold && (now - state.lastPunchTs) > 250){
      punchDetected = true; state.punchCount += 1; state.lastPunchTs = now; punchCountBadge.textContent = 'Punches: '+state.punchCount;
    }

    // Power estimate: power ~ speed * multiplier
    const powerEst = handSpeed * parseFloat(pow.value);

    // Accuracy: if a target circle exists, check wrist nearest to target; otherwise accuracy measured by straightness (wrist near line from shoulder to nose)
    const accuracy = computeAccuracy(last, canvas);

    // Guard: check both wrists distance to cheek/ear (nose approximates head center)
    const leftGuardDist = ndist(leftW, leftS);
    const rightGuardDist = ndist(rightW, rightS);
    const guardUp = (leftGuardDist < CONFIG.guardDistThreshold) && (rightGuardDist < CONFIG.guardDistThreshold);

    // Chin exposed: estimate angle from shoulders midpoint to nose; if nose is significantly forward/down relative to shoulders -> chin exposed
    const v = {x: (nose?.x ?? 0.5) - (midShoulder.x), y: (nose?.y ?? 0.4) - (midShoulder.y)};
    const headLen = Math.sqrt(v.x*v.x + v.y*v.y);
    const chinExposed = headLen > CONFIG.chinAngleThreshold && v.y > 0.04; // heuristic

    // Update UI metrics (scaled values)
    speedElem.textContent = handSpeed.toFixed(3);
    powerElem.textContent = powerEst.toFixed(3);
    accuracyElem.textContent = (accuracy*100).toFixed(0) + '%';
    guardElem.textContent = guardUp ? 'UP' : 'LOW';
    chinElem.textContent = chinExposed ? 'YES' : 'NO';

    if(punchDetected){
      // feedback sound based on power + accuracy
      if(accuracy > 0.6 && powerEst > 0.85) { beep(420,0.12,'square'); lastFeedback.textContent = 'Solid hit'; }
      else if(accuracy > 0.4){ beep(720,0.08,'sine'); lastFeedback.textContent = 'Good'; }
      else { beep(220,0.12,'sine'); lastFeedback.textContent = 'Miss'; }
    }

    // guard and chin warnings
    if(!guardUp){ beep(140,0.06,'sine'); }
    if(chinExposed){ beep(260,0.06,'triangle'); }

    return {punchDetected, handSpeed, powerEst, accuracy, guardUp, chinExposed, pose: last};
  }

  function computeAccuracy(historyEntry, canvasRef){
    // If showTarget is on: target is centered near upper center (opponent head). Use distance from wrist to target.
    if(!historyEntry) return 0;
    const target = {x: 0.5, y: 0.25}; // normalized
    const lw = historyEntry.leftW; const rw = historyEntry.rightW;
    const w = canvasRef.width, h = canvasRef.height;
    const dL = lw ? Math.sqrt((lw.x-target.x)**2 + (lw.y-target.y)**2) : 9;
    const dR = rw ? Math.sqrt((rw.x-target.x)**2 + (rw.y-target.y)**2) : 9;
    const d = Math.min(dL,dR);
    // convert to accuracy (0..1): perfect if d<0.08, 0 at d>0.5
    const acc = Math.max(0, 1 - (d - 0.02) / 0.48);
    return Math.min(1, Math.max(0, acc));
  }

  // ---------- Drawing helpers ----------
  function drawOverlay(pose, metrics){
    ctx.clearRect(0,0,canvas.width,canvas.height);
    // draw target
    if(showTarget.checked){
      const tx = canvas.width*0.5, ty = canvas.height*0.25, r= Math.max(24, canvas.width*0.06);
      ctx.beginPath(); ctx.fillStyle='rgba(30,120,255,0.18)'; ctx.arc(tx,ty,r,0,Math.PI*2); ctx.fill();
      ctx.beginPath(); ctx.strokeStyle='rgba(30,120,255,0.9)'; ctx.lineWidth=2; ctx.arc(tx,ty,r,0,Math.PI*2); ctx.stroke();
    }

    // draw skeleton
    if(!pose) return;
    const kp = pose.keypoints || [];
    ctx.lineWidth = 2; ctx.strokeStyle='rgba(255,255,255,0.7)'; ctx.fillStyle='rgba(255,255,255,0.9)';
    for(const k of kp){ const x = k.x/video.videoWidth*canvas.width; const y = k.y/video.videoHeight*canvas.height; ctx.beginPath(); ctx.arc(x,y,4,0,Math.PI*2); ctx.fill(); }
    // draw lines between common pairs
    const pairs = [['left_shoulder','right_shoulder'],['left_shoulder','left_elbow'],['left_elbow','left_wrist'],['right_shoulder','right_elbow'],['right_elbow','right_wrist'],['left_shoulder','left_hip'],['right_shoulder','right_hip']];
    function find(name){return kp.find(k=>k.name===name)}
    for(const p of pairs){ const a=find(p[0]), b=find(p[1]); if(a && b){ ctx.beginPath(); ctx.moveTo(a.x/video.videoWidth*canvas.width,a.y/video.videoHeight*canvas.height); ctx.lineTo(b.x/video.videoWidth*canvas.width,b.y/video.videoHeight*canvas.height); ctx.stroke(); } }

    // overlay text
    ctx.font = '16px system-ui'; ctx.fillStyle='rgba(255,255,255,0.85)';
    ctx.fillText('Punches: '+state.punchCount, 8, 20);
    if(metrics){ ctx.fillText('Speed:'+metrics.handSpeed.toFixed(3),8,40); ctx.fillText('Pow:'+metrics.powerEst.toFixed(3),8,60); }
  }

  // ---------- Main loop ----------
  async function detectLoop(){
    const now = performance.now();
    const poses = await detector.estimatePoses(video,{flipHorizontal:true});
    const pose = poses && poses[0] ? poses[0] : null;
    const metrics = pose ? analyze(pose, now) : null;
    drawOverlay(pose, metrics);

    // FPS calc
    const dt = now - state.lastTs; state.lastTs = now; const curF = 1000/dt;
    state.fps = CONFIG.fpsSmoothing*state.fps + (1-CONFIG.fpsSmoothing)*curF;
    fpsBadge.textContent = 'FPS: ' + state.fps.toFixed(1);

    rafId = requestAnimationFrame(detectLoop);
  }

  // ---------- Start / Stop controls ----------
  startBtn.addEventListener('click', async ()=>{
    startBtn.disabled = true; readyBadge.textContent = 'Loading model...';
    if(!detector){
      // load libraries via dynamic script insertion if not present
      if(!window.tf){
        const s1 = document.createElement('script'); s1.src='https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js'; document.head.appendChild(s1); await new Promise(r=>s1.onload=r);
      }
      if(!window.poseDetection){
        const s2 = document.createElement('script'); s2.src='https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection@0.0.8/dist/pose-detection.min.js'; document.head.appendChild(s2); await new Promise(r=>s2.onload=r);
      }
      // create MoveNet detector
      const detectorConfig = {modelType: poseDetection.movenet.modelType.SINGLEPOSE_LIGHTNING};
      detector = await poseDetection.createDetector(poseDetection.SupportedModels.MoveNet, detectorConfig);
    }

    await startCamera();
    sizeCanvas();
    readyBadge.textContent = 'Model: ready';
    stopBtn.disabled = false;
    running = true;
    state.history = []; state.punchCount = 0; state.lastPunchTs = 0;
    detectLoop();
  });

  stopBtn.addEventListener('click', ()=>{
    startBtn.disabled = false; stopBtn.disabled = true; readyBadge.textContent = 'Stopped';
    running = false; if(rafId) cancelAnimationFrame(rafId);
    // stop camera
    const stream = video.srcObject; if(stream){ stream.getTracks().forEach(t=>t.stop()); video.srcObject = null; }
  });

  // Resize handling
  window.addEventListener('resize', sizeCanvas);

  // Quick tap-to-unmute audio context on mobile
  document.body.addEventListener('click', ()=>{ if(audioCtx.state === 'suspended') audioCtx.resume(); }, {once:true});

  </script></body>
</html>    const canvas = document.getElementById('overlay');
    const ctx = canvas.getContext('2d');
    const targetEl = document.getElementById('target');

    const startBtn = document.getElementById('startBtn');
    const stopBtn = document.getElementById('stopBtn');
    const calibrateBtn = document.getElementById('calibrateBtn');
    const speedEl = document.getElementById('speed');
    const powerEl = document.getElementById('power');
    const punchTypeEl = document.getElementById('punchType');
    const accuracyEl = document.getElementById('accuracy');
    const chinEl = document.getElementById('chin');
    const guardEl = document.getElementById('guard');
    const debug = document.getElementById('debug');

    const speedThresh = document.getElementById('speedThresh');
    const powerMul = document.getElementById('powerMul');
    const accRadius = document.getElementById('accRadius');
    const speedVal = document.getElementById('speedVal');
    const powerVal = document.getElementById('powerVal');
    const accVal = document.getElementById('accVal');
    const soundOn = document.getElementById('soundOn');
    const soundOff = document.getElementById('soundOff');

    let detector = null;
    let running = false;
    let stream = null;
    let lastTimestamp = null;
    let prevKeypoints = null;
    let neutralPose = null;
    let useSound = true;
    let fallbackOnly = false; // if model fails

    // audio
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
    function playTone(freq=440,duration=0.08, type='sine'){ if(!useSound) return; try{ if(audioCtx.state==='suspended') audioCtx.resume(); const o = audioCtx.createOscillator(); const g = audioCtx.createGain(); o.type = type; o.frequency.value = freq; o.connect(g); g.connect(audioCtx.destination); g.gain.value = 0.0001; o.start(); g.gain.exponentialRampToValueAtTime(0.2, audioCtx.currentTime + 0.01); setTimeout(()=>{ g.gain.exponentialRampToValueAtTime(0.0001, audioCtx.currentTime + 0.02); o.stop(); }, duration*1000); }catch(e){ console.warn('Audio fail',e); }}

    function logDebug(msg){ debug.textContent = typeof msg === 'string' ? msg : JSON.stringify(msg); }

    // slider labels
    speedThresh.addEventListener('input',()=>speedVal.textContent = speedThresh.value);
    powerMul.addEventListener('input',()=>powerVal.textContent = powerMul.value);
    accRadius.addEventListener('input',()=>accVal.textContent = accRadius.value);
    soundOn.addEventListener('click',()=>{ useSound=true; soundOn.classList.remove('secondary'); soundOff.classList.add('secondary'); });
    soundOff.addEventListener('click',()=>{ useSound=false; soundOff.classList.remove('secondary'); soundOn.classList.add('secondary'); });

    // target move
    function moveTargetToClient(x,y){ const rect = targetEl.parentElement.getBoundingClientRect(); const tx = x - rect.left - targetEl.offsetWidth/2; const ty = y - rect.top - targetEl.offsetHeight/2; targetEl.style.left = tx+'px'; targetEl.style.top = ty+'px'; }
    targetEl.addEventListener('click',(e)=>{ moveTargetToClient(e.clientX,e.clientY); });
    canvas.addEventListener('click',(e)=>{ moveTargetToClient(e.clientX,e.clientY); });

    // camera
    async function startCamera(){
      if(stream) return;
      try{
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: 'user', width: { ideal: 1280 }, height: { ideal: 720 } }, audio:false });
        video.srcObject = stream;
        await video.play();
        resizeCanvas();
        logDebug('Camera started');
      }catch(err){ logDebug('Camera error: '+err.message); alert('Camera access denied or not available. Make sure you allow camera permission.
'+err.message); throw err; }
    }
    function stopCamera(){ if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; video.srcObject=null; logDebug('Camera stopped'); }}
    function resizeCanvas(){ canvas.width = video.videoWidth || video.clientWidth; canvas.height = video.videoHeight || video.clientHeight; }
    window.addEventListener('resize',resizeCanvas);

    // draw keypoints if available
    function drawKeypoints(keypoints){ ctx.clearRect(0,0,canvas.width,canvas.height); ctx.save(); ctx.scale(-1,1); ctx.translate(-canvas.width,0); ctx.lineWidth=2; for(const kp of keypoints){ if(kp.score>0.35){ ctx.beginPath(); ctx.arc(kp.x, kp.y, 4,0,Math.PI*2); ctx.fillStyle='rgba(6,182,212,0.9)'; ctx.fill(); }} const pairs = [[0,1],[1,3],[1,2],[2,4],[0,5],[5,7],[7,9],[5,6],[6,8],[8,10],[11,12],[12,14],[14,16],[11,13],[13,15]]; ctx.strokeStyle='rgba(255,255,255,0.08)'; for(const p of pairs){ const a=keypoints[p[0]], b=keypoints[p[1]]; if(a && b && a.score>0.35 && b.score>0.35){ ctx.beginPath(); ctx.moveTo(a.x,a.y); ctx.lineTo(b.x,b.y); ctx.stroke(); }} ctx.restore(); }

    function distance(a,b){ return Math.hypot(a.x-b.x,a.y-b.y); }

    function isChinExposed(keypoints){ const nose = keypoints.find(k=>k.name==='nose') || keypoints[0]; const leftShoulder = keypoints.find(k=>k.name==='left_shoulder'); const rightShoulder = keypoints.find(k=>k.name==='right_shoulder'); if(!nose || !leftShoulder || !rightShoulder) return false; const avgShoulderY = (leftShoulder.y + rightShoulder.y)/2; return (nose.y < avgShoulderY - 20); }
    function isGuardUp(keypoints){ const leftW = keypoints.find(k=>k.name==='left_wrist'); const rightW = keypoints.find(k=>k.name==='right_wrist'); const leftE = keypoints.find(k=>k.name==='left_eye'); const rightE = keypoints.find(k=>k.name==='right_eye'); const headY = ((leftE?.y||0)+(rightE?.y||0))/2 || 0; if(!leftW || !rightW || !headY) return false; return (leftW.y < headY + 80 && rightW.y < headY + 80); }

    // fallback motion detection when model not available: uses pixel motion of wrist-like areas (simple)

    // main detection loop
    async function runFrameWithPose(keypoints){ // keypoints: array with {x,y,score,name}
      drawKeypoints(keypoints);
      const now = performance.now(); if(!lastTimestamp) lastTimestamp = now; const dt = (now - lastTimestamp)/1000; lastTimestamp = now;
      if(prevKeypoints){
        const leftWPrev = prevKeypoints.find(k=>k.name==='left_wrist'); const rightWPrev = prevKeypoints.find(k=>k.name==='right_wrist'); const leftW = keypoints.find(k=>k.name==='left_wrist'); const rightW = keypoints.find(k=>k.name==='right_wrist');
        if(leftW && leftWPrev && rightW && rightWPrev){
          const lSpeed = Math.hypot((leftW.x-leftWPrev.x)/dt,(leftW.y-leftWPrev.y)/dt)/100;
          const rSpeed = Math.hypot((rightW.x-rightWPrev.x)/dt,(rightW.y-rightWPrev.y)/dt)/100;
          const topSpeed = Math.max(lSpeed,rSpeed);
          speedEl.textContent = topSpeed.toFixed(2);
          const threshold = parseFloat(speedThresh.value);
          let punchDetected=false; let punchSide=''; let punchSpeed=0; let wrist=null;
          if(lSpeed>threshold && lSpeed>rSpeed){ punchDetected=true; punchSide='Left'; punchSpeed=lSpeed; wrist=leftW; }
          else if(rSpeed>threshold && rSpeed>lSpeed){ punchDetected=true; punchSide='Right'; punchSpeed=rSpeed; wrist=rightW; }
          if(punchDetected){ punchTypeEl.textContent = punchSide + ' punch'; playTone(600,0.08,'square'); const powerEstimate = Math.round(punchSpeed * parseFloat(powerMul.value) * 100); powerEl.textContent = powerEstimate; const rect = canvas.getBoundingClientRect(); const tRect = targetEl.getBoundingClientRect(); const tx = tRect.left - rect.left + tRect.width/2; const ty = tRect.top - rect.top + tRect.height/2; const d = Math.hypot(wrist.x - tx, wrist.y - ty); const radius = parseInt(accRadius.value,10); let accText = Math.max(0, Math.round(100*(1 - Math.min(d,radius)/radius))); accuracyEl.textContent = accText + '%'; if(accText>75){ playTone(900,0.14,'sine'); } else if(accText>40){ playTone(700,0.09,'sine'); } else { playTone(300,0.12,'sawtooth'); }
          } else { punchTypeEl.textContent = '—'; powerEl.textContent = '0'; }
        }
      }
      const chin = isChinExposed(keypoints); chinEl.textContent = chin? 'Yes' : 'No'; chinEl.style.color = chin? 'var(--bad)':''; const guard = isGuardUp(keypoints); guardEl.textContent = guard? 'Up':'Down'; guardEl.style.color = guard? 'var(--good)':'var(--bad)'; prevKeypoints = keypoints;
    }

    // fallback simple loop if model unavailable — we approximate wrists by scanning for bright skin-tones would be hard; instead we just show video and no pose
    async function fallbackLoop(){ // crude: just update UI and continue
      running = true; logDebug('Running fallback motion-only (no pose model)'); while(running){ try{ // no model — set small heartbeat values
          speedEl.textContent = '0.00'; powerEl.textContent='0'; punchTypeEl.textContent='—'; await new Promise(r=>setTimeout(r,100)); }catch(e){ console.warn(e); } }
    }

    // choose model path
    async function loadPoseModel(){
      try{
        logDebug('Loading pose model...');
        const tfModule = await import('https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@3.21.0/dist/tf.min.js');
        const posedetection = await import('https://cdn.jsdelivr.net/npm/@tensorflow-models/pose-detection@0.0.7/dist/pose-detection.min.js');
        const model = posedetection.SupportedModels.MoveNet;
        const detectorConfig = {modelType: posedetection.movenet.modelType.SINGLEPOSE_LIGHTNING};
        detector = await posedetection.createDetector(model, detectorConfig);
        logDebug('Pose model loaded');
        fallbackOnly = false;
      }catch(err){ console.warn('Model load failed',err); logDebug('Model failed to load — running fallback only'); fallbackOnly = true; }
    }

    // run detection; supports both model-based and fallback
    async function runFrame(){ if(!running) return; resizeCanvas(); if(!fallbackOnly && detector){ try{ const poses = await detector.estimatePoses(video, {flipHorizontal:true}); if(poses && poses.length>0){ const pose = poses[0]; const keypoints = pose.keypoints.map(k=>({x:k.x,y:k.y,score:k.score,name:k.name})); await runFrameWithPose(keypoints); } else { // no pose found
            prevKeypoints=null; }
        }catch(err){ console.error('Detection error',err); logDebug('Detection error: '+err.message); }
      } else { // fallback
        // very lightweight loop - still keep UI responsive
        // We could implement a simple optical flow here, but that's complex; for now we run fallbackLoop
      }
      if(running) requestAnimationFrame(runFrame);
    }

    // Start/Stop/Calibrate
    startBtn.addEventListener('click', async ()=>{
      try{
        // resume audio on user gesture
        if(audioCtx.state==='suspended') await audioCtx.resume();
        await startCamera();
        await loadPoseModel();
        running=true; lastTimestamp=null; prevKeypoints=null; playTone(440,0.08);
        requestAnimationFrame(runFrame);
        logDebug('Running — model loaded: '+(!fallbackOnly));
      }catch(e){ logDebug('Start failed: '+e.message); }
    });

    stopBtn.addEventListener('click', ()=>{ running=false; stopCamera(); playTone(220,0.08); logDebug('Stopped'); });

    calibrateBtn.addEventListener('click', ()=>{ if(!prevKeypoints){ alert('Stand in your neutral guard and press Calibrate again'); return; } neutralPose = prevKeypoints; alert('Calibrated'); playTone(520,0.08); });

    // initial status
    logDebug('Ready — tap Start to allow camera & load model');
  });
  </script></body>
</html>
